**Optimization of Work-Stealing Dequeue for Weak Memory Models**
*5 December 2018*

If you've ever used Thread-Pool, you're probably familiar with concept of worker threads and tasks.
Whole idea boils down to writing programs as collection of tasks, that can be executed in parallel
on any of the worker threads. Those threads are assigned to CPU cores, and by processing unrelated
tasks they are trying to achieve maximum CPU utilization (by avoiding data sharing and locking).

Tasks are lightweight and execute on Fibers, which are mechanism of exposing cooperative scheduling
on given thread. This way tasks waiting for some event to happen, yeld worker thread to other tasks
in queue ensuring that CPU core is never stalled (worked threads always execute some tasks as long
as there are tasks available).

Core concept in Thread-Pool implementation is queue storing tasks to execute. While simple Thread-Pool
may have one queue shared by all threads, the whole idea is to keep worker threads separate, and
to minimize interaction between them. Thus standard approach is to have dedicated tasks queue per
thread. Threads interact with each other, only when they run out of tasks, by trying to "steal" some
tasks from other thread queues. 

This means that such queue implementation is crucial to whole system performance. It should be non-blocking
to ensure no worker thread is ever stalled, and allow fast way of adding as well as removing tasks. 
Thats where Work-Stealing Dequeue described in [Chase-Lev publication](
https://www.dre.vanderbilt.edu/~schmidt/PDF/work-stealing-dequeue.pdf target="_blank") comes in handy.

While I was implementing Thread-Pool in my hobby game engine, I've decided to use its more recent version
optimized for CPU architectures with weak memory model (like ARM) described in 
["Correct and Efficient Work-Stealing for Weak Memory Models"](https://www.di.ens.fr/~zappa/readings/ppopp13.pdf target="_blank").

<big>Bug</big>

During implementation of that algorithm I've noticed that it has bug. In original Chase-Lev publication
<code>top</code> and <code>bottom</code> variables are signed integers, yet in code listing in Figure 1.
of optimized version, those variables are unsigned integers (<code>atomic_size_t</code>). Treating them 
as unsigned causes <code>"B"</code> local variable to overflow to <code>ULLONG_MAX</code> value on take() 
operation when queue is empty, and thus it's never increased back, as it fails "if" conditions that follow. 
If both <code>bottom</code> and <code>top</code> are of signed type, then <code>"B"</code> goes to -1, 
and properly gets increaed back to 0 on empty queue.

!!! note 2nd February 2025
    It looks like few years later somebody else noticed the same bug:
    https://wingolog.org/archives/2022/10/03/on-correct-and-efficient-work-stealing-for-weak-memory-models

<big>Further optimization</big>

Now that algorithm is fixed, we can discuss further optimization. If memory that is backing array, is implemented 
using pre-allocated Virtual Memory address range, then that array can be grown by mapping additional memory 
pages, without need to re-allocate it. So it's base adress never changes, and bottom marker stays valid. 
This allows to simplify implementation by removing few atomic loads of array pointer. You can analyze 
final implementation in the code listing below:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
/*

 Ngine v5.0
 
 Module           : Work-Stealing Deque
 Multi-Threaded   : Safe
 Type             : Deque (Double-ended circular queue)
 Producers/Consum : Single-Producer / Multiple-Consumers (SPMC)
 Data structure   : Array-based (growing Virtual Memory, no relocation)
 Intrusiveness    : Intrusive (array based, so no extra allocations)
 Maximum size     : Unbounded (to defined VM limit)
 Overflow behavior: Fails on overflow (debug assert)
 Garbage collector: Not Required
 Priorities       : No support
 Ordering         : FIFO (for producer), no ordering (for consumers)
 Producer FPG     : Wait-freedom
 Consumer FPG     : Lock-freedom
 Expected usage   : A queue usually contains substantial amount of elements
 Failure behavior : Non-blocking (error message on empty/full**)
 Description      : Implementation of Chase-Lev "Dynamic Circular Work-Stealing Deque".
                    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.170.1097&rep=rep1&type=pdf
                
                    Implementation is based on below:
                    "Correct and Efficient Work-Stealing for Weak Memory Models"
                    https://www.di.ens.fr/~zappa/readings/ppopp13.pdf

                    ** TODO: Implement proper error message on full 

 FPG refers to Forward Progress Guarantee:
 http://www.1024cores.net/home/lock-free-algorithms/introduction
*/

#ifndef ENG_MEMORY_WORK_STEALING_DEQUE
#define ENG_MEMORY_WORK_STEALING_DEQUE

#include "core/defines.h"
#include "core/types.h"
#include "core/memory/pageAllocator.h"
#include <atomic>

namespace en
{

enum class DequeResult : uint8
{
    Success  = 0,
    Empty       ,
    Abort       ,
};
   
/*
template<typename T>
typedef struct 
{
    std::atomic_size_t size;
    std::atomic<T>*    buffer; // Array of elements of type T (ensures that elements of any size can be managed in multi-threaded non-blocking manner by managing pointers to them)
} Array;
//*/

template<typename T>
class WorkStealingDeque
{
private:
  //std::atomic<Array<T*>*> array; // Pointer to array (so that whole array can be atomically replaced after growing)
    std::atomic<sint64> top;    // Head of the queue
    std::atomic<sint64> bottom; // Tail of the queue
    std::atomic<T>*     buffer; // Backing memory (array of elements of type T)
    std::atomic<uint64> size;   // Current memory size in bytes (rounded up to multiple of 4KB)
    uint64 maxSize;             // Max memory allocation in bytes (rounded up to multiple of 4KB)
    uint64 doubleSizeUntil;     // Doubling allocation size barrier in bytes

    void resize(void);          // Asserts if maximum allowed capacity was reached
   
public:
    // First element starting address is always aligned to page size (4KB).
    // When deque is growing, it's size is doubling until reaching doubleSizeUntil 
    // size in bytes. After that, it always grows by that size, until reaching 
    // maxCapacity.
    WorkStealingDeque(const uint32 capacity,                               // In entries
                      const uint32 maxCapacity,                            // In entries
                      const uint32 doubleSizeUntil = PoolDoublingBarrier); // In bytes
   ~WorkStealingDeque();

    DequeResult take(T& value);   // Called only by owner thread
    void        push(T value);    // Called only by owner thread
    DequeResult steal(T& value);  // Can be called by any thread
};
 
static_assert(sizeof(WorkStealingDeque<void*>) == 48, "en::WorkStealingDeque<void*> size mismatch!");

template<typename T>
WorkStealingDeque<T>::WorkStealingDeque(
        const uint32 capacity, 
        const uint32 maxCapacity,
        const uint32 _doubleSizeUntil) :
    top(0),
    bottom(0),
    buffer(nullptr),
    size(roundUp(sizeof(T) * capacity, 4096)),
    maxSize(roundUp(sizeof(T) * maxCapacity, 4096)),
    doubleSizeUntil(_doubleSizeUntil)
{
    // Managed elements cannot exceed pointer size (max 8 bytes), to ensure that
    // they will be loaded and stored in atomic manner within single operation
    assert( sizeof(T) <= sizeof(void*) );

  //assert( powerOfTwo(sizeof(T)) );
 
    buffer = reinterpret_cast<std::atomic<T>*>(virtualAllocate(size, maxSize));
}

template<typename T>
WorkStealingDeque<T>::~WorkStealingDeque()
{
    virtualDeallocate(buffer, maxSize);
}

template<typename T>
void WorkStealingDeque<T>::resize(void)
{
    assert( size < maxSize );

    uint64 newSize = size < doubleSizeUntil ? size * 2 : roundUp(size + doubleSizeUntil, 4096);
    if (newSize > maxSize)
    {
        newSize = maxSize;
    }

    // Note [1]:
    //
    // Because circular deque is backed by virtual alocation, it's growing doesn't
    // affect data location in memory (preserving all pointers), nor deque logical 
    // structure (when deque is full, bottom index points at it's end, and after 
    // growing, it's pointing at next available element in memory).
    bool resize = virtualReallocate((void*)buffer, size, newSize);
    assert( resize );

    // Ensure that other threads during steal will see changed size 
    std::atomic_store_explicit(&size, newSize, std::memory_order_relaxed); // Added due to [1]
}

template<typename T>
DequeResult WorkStealingDeque<T>::take(T& value)
{
    sint64 b = std::atomic_load_explicit(&bottom, std::memory_order_relaxed) - 1;

    // (!)
    // This algorithm doesn't work if bottom and top are UNSIGNED.
    // Above subtraction changes b == INT_MAX, which break below conditions and as 
    // a result, is never increased. So type cannot be size_t (unsigned) but signed.
    // In original algorithm those types are also signed.

  //Array *a = std::atomic_load_explicit(&array, std::memory_order_relaxed);  // Optimized out due to [1]
    std::atomic_store_explicit(&bottom, b, std::memory_order_relaxed);
    std::atomic_thread_fence(std::memory_order_seq_cst);
    sint64 t = std::atomic_load_explicit(&top, std::memory_order_relaxed);
   
    DequeResult result = DequeResult::Success;
    T temp;
    if (t <= b)
    {
        // Non-empty queue
        uint32 capacity = static_cast<uint32>(size / sizeof(T));
        temp = std::atomic_load_explicit(&buffer[b % capacity], std::memory_order_relaxed);
        if (t == b)
        {
            // Single last element in queue
            if (!std::atomic_compare_exchange_strong_explicit(&top, &t, t + 1, std::memory_order_seq_cst, std::memory_order_relaxed))
            {
                result = DequeResult::Empty; // Failed race
            }
   
            std::atomic_store_explicit(&bottom, b + 1, std::memory_order_relaxed);
        }
    }
    else // Empty queue
    {
        result = DequeResult::Empty;
        std::atomic_store_explicit(&bottom, b + 1, std::memory_order_relaxed);
    }
   
    if (result == DequeResult::Success)
    {
        value = temp;
    }
   
    return result;
}
   
template<typename T>
void WorkStealingDeque<T>::push(T value)
{
    sint64 b = std::atomic_load_explicit(&bottom, std::memory_order_relaxed);
    sint64 t = std::atomic_load_explicit(&top, std::memory_order_acquire);
  //Array *a = std::atomic_load_explicit(&array, std::memory_order_relaxed);      // Optimized out due to [1]
    uint32 capacity = static_cast<uint32>(size / sizeof(T));
    if (b - t > capacity - 1)
    {
        // Full queue
        resize();

      //a = std::atomic_load_explicit(&array, std::memory_order_relaxed);          // Optimized out due to [1]
        capacity = static_cast<uint32>(size / sizeof(T));
    }

    std::atomic_store_explicit(&buffer[b % capacity], value, std::memory_order_relaxed);
    std::atomic_thread_fence(std::memory_order_release);
    std::atomic_store_explicit(&bottom, b + 1, std::memory_order_relaxed);
}
   
template<typename T>
DequeResult WorkStealingDeque<T>::steal(T& value)
{
    sint64 t = std::atomic_load_explicit(&top, std::memory_order_acquire);
    std::atomic_thread_fence(std::memory_order_seq_cst);
    sint64 b = std::atomic_load_explicit(&bottom, std::memory_order_acquire);
   
    DequeResult result = DequeResult::Empty;
    T temp;
    if (t < b)
    {
        // Non-empty queue
      //Array *a = std::atomic_load_explicit(&array, std::memory_order_consume);             // Optimized out due to [1]
        uint64 currentSize = std::atomic_load_explicit(&size, std::memory_order_consume);    // Added due to [1]
        uint32 capacity = static_cast<uint32>(currentSize / sizeof(T));
        temp = std::atomic_load_explicit(&buffer[t % capacity], std::memory_order_relaxed);
        result = DequeResult::Success;
        if (!std::atomic_compare_exchange_strong_explicit(&top, &t, t + 1, std::memory_order_seq_cst, std::memory_order_relaxed))
        {
            result = DequeResult::Abort; // Failed race
        }
    }
   
    if (result == DequeResult::Success)
    {
        value = temp;
    }
   
    return result;
}

}
#endif
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For completness, this is the virtual memory allocator interface:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
/*

 Ngine v5.0
 
 Module      : Page Allocator.
 Requirements: none
 Description : Allocates memory and reserver adress space range for future 
               growth of allocation on page granularity.

*/

#ifndef ENG_CORE_MEMORY_PAGE_ALLOCATOR
#define ENG_CORE_MEMORY_PAGE_ALLOCATOR

#include "core/defines.h"
#include "core/types.h"

namespace en
{
   // Allocated memory is always aligned to 4KB
   void* virtualAllocate(const uint64 size, const uint64 maximumSize);
 
   // Increases allocation size without copying data, or changing location
   bool  virtualReallocate(void* address, const uint64 currentSize, const uint64 newSize);

   void  virtualDeallocate(void* address, const uint64 maximumSize);
}
#endif
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>